import numpy as np
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import joblib
import os
import json
from django.conf import settings


class GRUPeriodPredictor:
    def __init__(self):
        self.model = None
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.sequence_length = 6
        self.model_dir = os.path.join(settings.BASE_DIR, 'gru_models')
        os.makedirs(self.model_dir, exist_ok=True)

    def get_user_model_path(self, user_id):
        return os.path.join(self.model_dir, f'user_{user_id}')

    def create_features(self, records):
        """ä»ç»æœŸè®°å½•åˆ›å»ºç‰¹å¾"""
        if len(records) < 2:
            return None, None

        sorted_records = sorted(records, key=lambda x: x.start_date)
        cycle_lengths = []

        for i in range(1, len(sorted_records)):
            prev_start = sorted_records[i - 1].start_date
            curr_start = sorted_records[i].start_date
            days_between = (curr_start - prev_start).days

            if 20 <= days_between <= 45:
                cycle_lengths.append(days_between)

        if len(cycle_lengths) < self.sequence_length + 1:
            return None, None

        features = []
        for i in range(len(cycle_lengths) - self.sequence_length):
            sequence = cycle_lengths[i:i + self.sequence_length]
            feature_vector = list(sequence)

            # ç»Ÿè®¡ç‰¹å¾
            feature_vector.extend([
                np.mean(sequence), np.std(sequence),
                min(sequence), max(sequence), np.median(sequence)
            ])

            # è¶‹åŠ¿ç‰¹å¾
            if len(sequence) >= 2:
                trend = sequence[-1] - sequence[-2]
                feature_vector.append(trend)
            else:
                feature_vector.append(0)

            features.append(feature_vector)

        targets = cycle_lengths[self.sequence_length:]
        return np.array(features), np.array(targets)

    def build_model(self, input_shape):
        """æ„å»ºGRUæ¨¡å‹"""
        model = Sequential([
            GRU(50, return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            GRU(50, return_sequences=False),
            Dropout(0.2),
            Dense(25),
            Dense(1)
        ])

        model.compile(optimizer=Adam(learning_rate=0.001),
                      loss='mse',
                      metrics=['mae'])
        return model

    def train_model(self, user_id, records):
        """è®­ç»ƒGRUæ¨¡å‹"""
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒç”¨æˆ·{user_id}çš„GRUæ¨¡å‹")

        X, y = self.create_features(records)
        if X is None or len(X) < 3:
            print(f"âŒ ç”¨æˆ·{user_id}æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒGRUæ¨¡å‹")
            return False

        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))

        # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
        self.model = self.build_model((1, X_scaled.shape[1]))

        history = self.model.fit(
            X_reshaped, y,
            epochs=100,
            batch_size=16,
            validation_split=0.2,
            verbose=0,
            callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]
        )

        # ä¿å­˜æ¨¡å‹
        model_path = self.get_user_model_path(user_id)
        self.model.save(f"{model_path}.h5")
        joblib.dump(self.scaler, f"{model_path}_scaler.pkl")

        train_mae = history.history['mae'][-1]
        print(f"âœ… ç”¨æˆ·{user_id}çš„GRUæ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒMAE: {train_mae:.2f}å¤©")
        return True

    def load_model(self, user_id):
        """åŠ è½½ç”¨æˆ·æ¨¡å‹"""
        model_path = self.get_user_model_path(user_id)
        model_file = f"{model_path}.h5"
        scaler_file = f"{model_path}_scaler.pkl"

        if os.path.exists(model_file) and os.path.exists(scaler_file):
            try:
                self.model = tf.keras.models.load_model(model_file)
                self.scaler = joblib.load(scaler_file)
                return True
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return False

    def predict_next_cycle(self, user_id, records):
        """ä½¿ç”¨GRUé¢„æµ‹ä¸‹ä¸€ä¸ªå‘¨æœŸé•¿åº¦"""
        if not self.load_model(user_id):
            if not self.train_model(user_id, records):
                return self.fallback_prediction(records)

        X, _ = self.create_features(records)
        if X is None or len(X) == 0:
            return self.fallback_prediction(records)

        # ä½¿ç”¨æœ€æ–°åºåˆ—é¢„æµ‹
        latest_sequence = X[-1:].reshape(1, -1)
        latest_sequence_scaled = self.scaler.transform(latest_sequence)
        latest_sequence_reshaped = latest_sequence_scaled.reshape((1, 1, latest_sequence_scaled.shape[1]))

        prediction = self.model.predict(latest_sequence_reshaped, verbose=0)[0][0]
        predicted_cycle = int(round(max(20, min(45, prediction))))

        print(f"ğŸ¤– GRUé¢„æµ‹å‘¨æœŸé•¿åº¦: {predicted_cycle}å¤©")
        return predicted_cycle

    def fallback_prediction(self, records):
        """å›é€€åˆ°åŠ æƒå¹³å‡æ³•"""
        from . import calculate_weighted_average_cycle
        return calculate_weighted_average_cycle(records)


# å…¨å±€GRUé¢„æµ‹å™¨å®ä¾‹
gru_predictor = GRUPeriodPredictor()


def get_three_stage_predictions(user, records, profile, year, month):
    """
    ä¸‰é˜¶æ®µé¢„æµ‹ç®—æ³•ï¼š
    é˜¶æ®µ1 (1-3å‘¨æœŸ): å›ºå®šå‘¨æœŸ
    é˜¶æ®µ2 (4-6å‘¨æœŸ): åŠ æƒå¹³å‡
    é˜¶æ®µ3 (7+å‘¨æœŸ): GRUç¥ç»ç½‘ç»œ
    """
    print(f"=== ä¸‰é˜¶æ®µé¢„æµ‹ç®—æ³•å¯åŠ¨ ===")

    # è·å–å®é™…è®°å½•
    actual_records = [r for r in records if not r.is_predicted]
    if not actual_records:
        return [], []

    sorted_actual = sorted(actual_records, key=lambda x: x.start_date)
    cycle_count = len(sorted_actual) - 1

    print(f"ğŸ“Š è®°å½•åˆ†æ: {len(actual_records)}ä¸ªè®°å½•, {cycle_count}ä¸ªå®Œæ•´å‘¨æœŸ")

    # ä½¿ç”¨æœ€æ–°è®°å½•ä½œä¸ºå‚è€ƒ
    latest_record = sorted_actual[-1]
    reference_date = latest_record.end_date

    # ä¸‰é˜¶æ®µç®—æ³•é€‰æ‹©
    if cycle_count < 3:
        # é˜¶æ®µ1ï¼šå›ºå®šå‘¨æœŸ
        cycle_length = profile.cycle_length
        method = f"å›ºå®šå‘¨æœŸï¼ˆ{cycle_count}ä¸ªå‘¨æœŸï¼‰"
    elif cycle_count < 7:
        # é˜¶æ®µ2ï¼šåŠ æƒå¹³å‡
        cycle_length = calculate_weighted_average_cycle(sorted_actual)
        method = f"åŠ æƒå¹³å‡ï¼ˆ{cycle_count}ä¸ªå‘¨æœŸï¼‰"
    else:
        # é˜¶æ®µ3ï¼šGRUç¥ç»ç½‘ç»œ
        try:
            cycle_length = gru_predictor.predict_next_cycle(user.id, sorted_actual)
            method = f"GRUç¥ç»ç½‘ç»œï¼ˆ{cycle_count}ä¸ªå‘¨æœŸï¼‰"
        except Exception as e:
            print(f"âŒ GRUé¢„æµ‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°åŠ æƒå¹³å‡")
            cycle_length = calculate_weighted_average_cycle(sorted_actual)
            method = f"åŠ æƒå¹³å‡ï¼ˆå›é€€ï¼‰"

    period_length = profile.period_length

    print(f"ğŸ”§ é¢„æµ‹æ–¹æ³•: {method}")
    print(f"â±ï¸ é¢„æµ‹å‘¨æœŸ: {cycle_length}å¤©")

    # è®¡ç®—é¢„æµ‹å‘¨æœŸ
    prediction_start = reference_date + timedelta(days=cycle_length)
    prediction_end = prediction_start + timedelta(days=period_length - 1)

    next_prediction_start = prediction_start + timedelta(days=cycle_length)
    next_prediction_end = next_prediction_start + timedelta(days=period_length - 1)

    # ç”Ÿæˆç›®æ ‡æœˆä»½å†…çš„æ—¥æœŸ
    current_dates = generate_dates_in_month(prediction_start, prediction_end, year, month)
    next_dates = generate_dates_in_month(next_prediction_start, next_prediction_end, year, month)

    print(f"âœ… å½“å‰é¢„æµ‹åœ¨ç›®æ ‡æœˆä»½å†…: {len(current_dates)}å¤©")
    print(f"âœ… ä¸‹æ¬¡é¢„æµ‹åœ¨ç›®æ ‡æœˆä»½å†…: {len(next_dates)}å¤©")

    return current_dates, next_dates


def calculate_weighted_average_cycle(records):
    """è®¡ç®—åŠ æƒå¹³å‡å‘¨æœŸé•¿åº¦"""
    if len(records) < 2:
        return 28

    cycle_lengths = []
    for i in range(1, len(records)):
        prev_start = records[i - 1].start_date
        curr_start = records[i].start_date
        days_between = (curr_start - prev_start).days

        if 20 <= days_between <= 45:
            cycle_lengths.append(days_between)

    if not cycle_lengths:
        return 28

    # åŠ æƒå¹³å‡ï¼šè¿‘æœŸæ•°æ®æƒé‡æ›´é«˜
    n = len(cycle_lengths)
    weights = [0.5 ** (n - i - 1) for i in range(n)]
    total_weight = sum(weights)
    normalized_weights = [w / total_weight for w in weights]

    weighted_avg = sum(length * weight for length, weight in zip(cycle_lengths, normalized_weights))
    return int(round(max(20, min(45, weighted_avg))))


def generate_dates_in_month(start_date, end_date, year, month):
    """ç”ŸæˆæŒ‡å®šæœˆä»½å†…çš„æ—¥æœŸåˆ—è¡¨"""
    target_start = datetime(year, month, 1).date()
    if month == 12:
        target_end = datetime(year + 1, 1, 1).date() - timedelta(days=1)
    else:
        target_end = datetime(year, month + 1, 1).date() - timedelta(days=1)

    if end_date < target_start or start_date > target_end:
        return []

    overlap_start = max(start_date, target_start)
    overlap_end = min(end_date, target_end)

    dates = []
    current_date = overlap_start
    while current_date <= overlap_end:
        dates.append(current_date)
        current_date += timedelta(days=1)

    return dates